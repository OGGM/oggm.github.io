---
layout: post
title: The Global Ice Thickness Initiative (G2TI)
subtitle: ... or calibration until insanity
author: Fabien Maussion
date: 2018-05-21T00:00:00
tags: itmix science
---

A major objective of the [ACS Working Group on Glacier Ice Thickness Estimation](http://www.cryosphericsciences.org/wg_glacierIceThickEst.html)
is to provide an *"estimated ice thickness distribution for every glacier included
in the RGI"*. After a comparison of ice thickness estimation models on a handful of glaciers (
[Ice Thickness Models Intercomparison eXperiment, phase 1](https://www.the-cryosphere.net/11/949/2017/)
), the models capable to estimate ice thickness on a large number of glaciers
with limited input data where invited to participate (see the official announcement
[here](http://people.ee.ethz.ch/~danielfa/IACS/G2TI.html)).

A perfect job for OGGM, right? At least this is what I thought one year ago when
I pushed Daniel to go on with his plans, right in time for the upcoming IPCC
deadlines.  As it turns out (I should be used to it by now), the job was harder
than I previously thought. Not because OGGM cannot estimate the ice thickness
of any glacier (we've shown that [it can](https://www.geosci-model-dev-discuss.net/gmd-2018-9/)),
but because the actual question is: how good can we do that? OGGM performed
quite favorably in ITMIX1, but we are playing in another league here.

This document summarizes the major steps undertaken for this goal, and
will be concluded by many (many) open questions on how to go forward from here.

## Step 1: ingesting G2TI data into OGGM

*(spoiler: we failed)*

The input data provided for the experiment was, for each glacier:
- the glacier outlines from the RGI version 6
- a local topography map nicely prepared by Matthias

And, for 1087 of them, calibration/validation data in the form of point GPR
measurements (808 glaciers) or glacier averaged ice thickness (337). We'll
get back to these later.

OGGM is a standalone tool and doesn't need this input, but the point of the
experiment was to at least agree on something: the topography and the glacier
masks. Unfortunately, this didn't work well for us, for a couple or reasons
which weren't impossible to tackle but which I chose not to.

**First**, the provided data wasn't exactly extracted from RGI Version 6: the outlines
are from the beta version and still have an RGI id version 5. This was a minor
problem, because I just had to replace `50` with `60`. I noted some other
differences though: in at least one RGI region (RGI Region 02), the number of
glaciers didn't match exactly between both versions. So far so good, no big
deal, we can work with that.

**Second**, Matthias and OGGM of course have different opinions about which
resolution is best for the local DEM map. OGGM's default is to use a
square root function of the glacier size:

$$ \Delta x \textrm{[m]} = a \, \sqrt{\textrm{area} \textrm{[km]}} + b $$

with a = 14 and b = 10. When the resolution $\Delta x$ exceeds 200 m, is is
caped to that value. This choice wasn't driven by a real quantitative analysis,
I just had a look at a set of glaciers and it looked OK to me. Now let's
compare this to the data provided by Matthias:

<img src="/img/blog/g2ti/dem_res.svg" alt="" width="100%" />

So the clue here is that G2TI uses a step function (with steps 25, 50, 100, and
200 m), while OGGM (and this is the most important) is much faster in reaching
the 200 m mark. I am not going to argue here which method is best, but what is
sure is that because of computational efficiency OGGM cannot really use such
high resolution for very large glaciers. Inversely, for very small glaciers
(< 1km$^2$), OGGM has a finer resolution than G2TI.

Altogether I might have been able to use workarounds (like reprojecting G2TI
data into an OGGM map), if it wasn't for point 3 and 4 below.

**Third**, the G2TI data is a bit "noisy" for resolutions below that from the
original data. It's not very visible when looking at the DEM itself (left
plot below), but it is clear when computing the slope (right plot). For comparison,
I plotted the slope computed using a DEM generated on the same map with
[GDAL](http://www.gdal.org/).

<img src="/img/blog/g2ti/G2TI_dem.svg" alt="" width="38%" align="left"/>
<img src="/img/blog/g2ti/G2TI_slope.svg" alt="" width="55%" />

Again, it's not a big deal because we smooth the data *a lot* in OGGM (like all
other models), but all this together summed up to a series of annoyances I didn't
want to deal with. So I decided to use the standard OGGM workflow and
reproject everything back to the G2TI map at the very last step.

## Step 2: decide how to compute the distributed ice thickness maps

OGGM prime objective is to model glaciers, not to provide distributed ice
thickness maps. Similar to Farinotti et al. (2009) and Huss & Farinotti (2012),
we have to "trick" to convert the 1D "flowline" glaciers back to 2D bed
topography maps. For ITMIX1 I coded this part of the code in one afternoon,
and I promised myself I was trying to do better for G2TI. In the end, I'm
quite sure I didn't really manage.

The core of the problem is that this backwards step (from flowline to 2D) is
statistical in essence and has very little physical meaning. There are multiple
ways to do it, and I tried two major paths:
- treating the computed thickness at the centerline as "true" and interpolating
  between the centerline and the glacier boundaries. This works quite well for
  glaciers where the centerlines are well defined.
- assigning the centerlines thicknesses to the 2D glacier according to elevation
  bands and adding a scaling factor depending on the distance from the glacier
  outline. This is more robust for poorly defined centerlines.

Here is an example at the South Glacier (see ITMIX paper for details):

<img src="/img/blog/g2ti/maps_interp.png" alt="" width="100%" />

There are quite large differences between the methods. In this case (and, as
it turns out, in most cases), the altitudinal band method works best. Here is
how it really performs thought:

<img src="/img/blog/g2ti/thick_scatter.svg" alt="" width="70%" />

And this is one of the good examples. There are still obvious problems (like the
overestimation close to the glacier boundaries), and it is surely possible to
tweak the model towards better results for this one glacier (e.g. by acting on
the distance mask). However, I wanted to work in a much more systematic way.

And here starts the trouble.

I tested a lot of different parameter combinations: for each of the two
methods, I varied the influence of the distance to the boundaries,
the radius of the smoothing applied to the final thickness map, and I varied
Glen's flow law parameter A.

Because this step was about deciding the interpolation parameters only,
I made two decisions:
- I manually selected 156 glaciers which I thought had representative GPR
  data: that is, many points well distributed among the glacier. This makes
  kind of sense because I didn't want to take decisions based on a couple of
  GPR points at the glacier tongue only.
- To remove the effect of systematic bias, I would keep only the parameter
  combinations where the
  initial calibration of Glen's A (which controls the average thickness of the
  glacier) was successful (i.e. average bias close to zero).

For example at glacier RGI60-01.16195:

<img src="/img/blog/g2ti/choice_params.svg" alt="" width="100%" />

The shape of the curve just shows the different parameter combinations for
the altitudinal band method and for increasing A
(left part of the parameter range), and different parameter combinations for
the standard interpolation method. In green the selected points for further
analysis based on bias only. From these, I selected the best performing
with respect to Mean Absolute Deviation (MAD).

I repeated this step for all 131 glaciers (156 minus the 25 where the A
calibration failed), and finally counted the number of times a certain
interpolation strategy would work best. The strategy which performed
most often was the altitudinal band method with
a smoothing radius of 250 m and a distance to border mask elevated at the power
1/4. These are now the default parameters in OGGM.

**Note:** I repeated this with the 3-fold cross-validation step imposed by
G2TI, and it was always the same strategy which performed best in all cases.
Or, I should say, "less worse".

## Step 3: calibrate Glen's A

While the interpolation strategy surely plays an important role once the total
volume of the glacier is guessed correctly, it is the latter step which is
extremely difficult to get correctly.

In OGGM we use Glen's A as calibration parameter for the total glacier volume
because it is known to vary between glaciers and because it has a strong
influence on glacier volume. It must be noted however that we use it mostly as
a tuning parameter here, and that its physical meaning shouldn't be
over-interpreted.

As a reminder, here is a plot of global glacier volume as a function of a
multiplication factor applied to the default Glen parameter A (the plot is
from our [GMD paper](https://www.geosci-model-dev-discuss.net/gmd-2018-9/)):

<img src="/img/blog/g2ti/global_inv.png" alt="" width="100%" />

The question I wanted to answer in step 3 is: **can we pinpoint A to a certain
value so that our global estimates of ice volume are better constrained?**

*(spoiler: I don't think I managed, but I did my best in the time that was
allocated to me)*

### Ideal: find a way to calibrate A as a function of \<.\>

I tried to plot the "ideal A" as a function of many (many) things, leading to
the production of many cloud points. Nothing seemed to correlate, with one
exception maybe: the plot below shows the "ideal factor" for A as
a function of glacier area.

<img src="/img/blog/g2ti/scatter_area.svg" alt="" width="100%" />

From the plot it seems that smaller glaciers can be over- or under-estimated,
while large glaciers cannot. However, the data is way to uncertain to make
any rule of A based on glacier size.

### Less ideal: fix A to a compromise value

**Assuming that the glaciers with GPR data are representative for all glaciers
worldwide** (which they aren't), let's just pick an A so that the mean error
is minimized for all glaciers. This seems quite simple, but this leads to
many new questions without answer:
- should all glaciers have the same weight?
- if not, what is important? Glacier size? Number of GPR points?
- which score do we minimize? MAD? RMSD? BIAS?

Here is an example of the influence of this minimization process applied
to MAD and with different procedures: standard average, average weighted by
glacier area, average weighted by number of GPR points. We repeat the operation
for MAD, BIAS, and again for the 156 previously selected glaciers. The plot
below shows the minimization procedure, each black line representing a possible
optimum.

<img src="/img/blog/g2ti/perf.svg" alt="" width="100%" />

We find a range of "best factors" varying from 1.2 to 2.5. A value higher than
one is physically reasonable, since we neglect basal sliding and we assume
equilibrium (higher A means higher velocities and thinner glaciers).
But, from all these values, which one should we chose?

### Pragmatic: just pick one and stop calibrating

My final choice: **let's pick the A factor minimizing the area-weighted average
of MAD for the reference glaciers only.**

This gives us the following results:
- fA = 1.5 for the final run
- fA = 1.7 for the 3-fold cross-validation set 1
- fA = 1.3 for the 3-fold cross-validation set 2
- fA = 1.0 for the 3-fold cross-validation set 3

### How bad are we with this setup?

Let's leave this for Daniel to find out. My gut feeling is not so good:
we still have plenty to learn, and I believe that with dedicated research we
can improve OGGM a lot.

## Step 4: convert OGGM maps back to G2TI

TODO
